{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/')\n",
    "import yaml\n",
    "import torch\n",
    "# from models_.lvmogp_svi import LVMOGP_SVI\n",
    "from modules.prepare_and_train_model import LVMOGP_SVI \n",
    "from models_.gaussian_likelihood import GaussianLikelihood\n",
    "from modules.prepare_data import *\n",
    "from util_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianLikelihood(\n",
       "  (noise_covar): HomoskedasticNoise(\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify model and likelihood\n",
    "\n",
    "with open('/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/configs/train_lvmogp_config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "my_model = LVMOGP_SVI(\n",
    "    n_outputs = config['n_outputs'],\n",
    "    n_input = config['n_input_train'],\n",
    "    input_dim = config['input_dim'],\n",
    "    latent_dim = config['latent_dim'],\n",
    "    n_inducing_input = config['n_inducing_input'],\n",
    "    n_inducing_latent = config['n_inducing_latent'],\n",
    "    data_Y = None,\n",
    "    pca = config['pca'],\n",
    "    learn_inducing_locations_latent = config['learn_inducing_locations_latent'],\n",
    "    learn_inducing_locations_input = config['learn_inducing_locations_input'],\n",
    "    latent_kernel_type = config['latent_kernel_type'],\n",
    "    input_kernel_type = config['input_kernel_type']\n",
    ")\n",
    "\n",
    "my_likelihood = GaussianLikelihood()\n",
    "\n",
    "# Load trained model and likelihood\n",
    "\n",
    "model_state_dict = torch.load(config['model_path'])\n",
    "my_model.load_state_dict(model_state_dict)\n",
    "\n",
    "likelihood_state_dict = torch.load(config['likelihood_path'])\n",
    "my_likelihood.load_state_dict(likelihood_state_dict)\n",
    "\n",
    "my_model.eval()\n",
    "my_likelihood.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variational_strategy.inducing_points_latent\n",
      "variational_strategy.inducing_points_input\n",
      "variational_strategy._variational_distribution.variational_mean\n",
      "variational_strategy._variational_distribution.chol_variational_covar_latent\n",
      "variational_strategy._variational_distribution.chol_variational_covar_input\n",
      "X.q_mu\n",
      "X.q_log_sigma\n",
      "covar_module_latent.raw_outputscale\n",
      "covar_module_latent.base_kernel.raw_lengthscale\n",
      "covar_module_input.raw_outputscale\n",
      "covar_module_input.base_kernel.raw_lengthscale\n"
     ]
    }
   ],
   "source": [
    "for name, param in my_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(my_model.covar_module_latent.outputscale.data)\n",
    "# print(my_model.covar_module_latent.base_kernel.lengthscale.data.reshape(-1))\n",
    "\n",
    "# kernel(h_1, h_2) = outputscale * exp(-0.5 * [ (h_11 - h_21 / lengthscale_1)^2 + (h_12 - h_22 / lengthscale_2)^2 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import CholLinearOperator, KroneckerProductLinearOperator\n",
    "from linear_operator.operators import (\n",
    "    LinearOperator,\n",
    "    TriangularLinearOperator,\n",
    ")\n",
    "from gpytorch.settings import _linalg_dtype_cholesky\n",
    "\n",
    "def _cholesky_factor(induc_induc_covar: LinearOperator) -> TriangularLinearOperator:\n",
    "    L = psd_safe_cholesky(to_dense(induc_induc_covar).type(_linalg_dtype_cholesky.value()), max_tries=4)\n",
    "    return TriangularLinearOperator(L)\n",
    "\n",
    "K_uu_latent = my_model.covar_module_latent(my_model.variational_strategy.inducing_points_latent.data).to_dense().to(torch.float64)\n",
    "K_uu_latent_inv = torch.linalg.solve(K_uu_latent, torch.eye(K_uu_latent.size(-1)).to(torch.float64))\n",
    "K_uu_input = my_model.covar_module_input(my_model.variational_strategy.inducing_points_input.data).to_dense().to(torch.float64)\n",
    "K_uu_input_inv = torch.linalg.solve(K_uu_input, torch.eye(K_uu_input.size(-1)).to(torch.float64))\n",
    "\n",
    "K_uu = KroneckerProductLinearOperator(K_uu_latent, K_uu_input).to_dense().data\n",
    "# chol_K_uu_inv_t = _cholesky_factor_latent(KroneckerProductLinearOperator(K_uu_latent_inv, K_uu_input_inv)).to_dense().data.t()\n",
    "chol_K_uu_inv_t = KroneckerProductLinearOperator(\n",
    "        torch.linalg.solve( _cholesky_factor(K_uu_latent).to_dense().data, torch.eye(K_uu_latent.size(-1)).to(torch.float64)),\n",
    "        torch.linalg.solve( _cholesky_factor(K_uu_input).to_dense().data, torch.eye(K_uu_input.size(-1)).to(torch.float64)),\n",
    "    ).to_dense().data.t()\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "chol_covar_latent_u = my_model.variational_strategy._variational_distribution.chol_variational_covar_latent.data.to(torch.float64)\n",
    "covar_latent_u = CholLinearOperator(chol_covar_latent_u).to_dense()\n",
    "chol_covar_input_u = my_model.variational_strategy._variational_distribution.chol_variational_covar_input.data.to(torch.float64)\n",
    "covar_input_u = CholLinearOperator(chol_covar_input_u).to_dense()\n",
    "\n",
    "covar_u = KroneckerProductLinearOperator(covar_latent_u, covar_input_u).to_dense().data\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "common_background_information = {\n",
    "                    'K_uu': K_uu.data,\n",
    "                    'chol_K_uu_inv_t': chol_K_uu_inv_t.data, \n",
    "                    'm_u': my_model.variational_strategy._variational_distribution.variational_mean.data,\n",
    "                    'Sigma_u': covar_u.data,\n",
    "                    'A': chol_K_uu_inv_t @ (covar_u - torch.eye(covar_u.shape[0])) @ chol_K_uu_inv_t.t(),\n",
    "                    'var_H': my_model.covar_module_latent.outputscale.data,\n",
    "                    'var_X': my_model.covar_module_input.outputscale.data,\n",
    "                    'W': my_model.covar_module_latent.base_kernel.lengthscale.data.reshape(-1)**2\n",
    "                    }\n",
    "'''\n",
    "chol_K_uu_inv_t: cholesky of inverse of K_uu matrix, of shape (M_H * M_X, M_H * M_X)\n",
    "m_u: mean of the variational distribution\n",
    "Sigma_u: covariance matrix of the variational distribution\n",
    "A: chol_K_uu_inv_t (Sigma_u - I) chol_K_uu_inv_t.T\n",
    "var_H: \n",
    "var_X: \n",
    "W: vector; containing all lengthscales in the RAD kernel\n",
    "c: constant\n",
    "'''\n",
    "c = (2 * torch.pi)**(config['latent_dim'] / 2) * common_background_information['var_H'] * common_background_information['W'].sqrt().prod()\n",
    "common_background_information['constant_c'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration_prediction_func(test_input, output_index, my_model, common_background_information):\n",
    "\n",
    "    input_K_f_u = my_model.covar_module_input(test_input, my_model.variational_strategy.inducing_points_input.data).to_dense().data\n",
    "    input_K_u_f_K_f_u = input_K_f_u.t() @ input_K_f_u\n",
    "\n",
    "    data_specific_background_information = {\n",
    "            'm_plus': my_model.X.q_mu.data[output_index],\n",
    "            'Sigma_plus': my_model.X.q_log_sigma.exp().square().data[output_index],\n",
    "            'input_K_f_u': input_K_f_u, \n",
    "            'input_K_u_f_K_f_u': input_K_u_f_K_f_u,\n",
    "            'expectation_K_uu': None\n",
    "            }\n",
    "    \n",
    "    # helper functions -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def multivariate_gaussian_pdf(x, mu, cov):\n",
    "        '''cov is a vector, representing all elements in the diagonal matrix'''\n",
    "        k = mu.size(0)\n",
    "        cov_det = cov.prod()\n",
    "        cov_inv = torch.diag(1.0 / cov)\n",
    "        norm_factor = torch.sqrt((2 * torch.pi) ** k * cov_det)\n",
    "\n",
    "        x_mu = x - mu\n",
    "        result = torch.exp(-0.5 * x_mu @ cov_inv @ x_mu.t()) / norm_factor\n",
    "        return result.item()\n",
    "\n",
    "    def G(h:Tensor, common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "\n",
    "        mu = data_specific_background_information['m_plus']\n",
    "        cov_diag = data_specific_background_information['Sigma_plus'] + common_background_information['W']\n",
    "        result = multivariate_gaussian_pdf(h, mu, cov_diag)\n",
    "        return common_background_information['constant_c'] * result\n",
    "\n",
    "    def R(h_1:Tensor, h_2:Tensor, common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        mu_1 = h_2\n",
    "        cov_diag_1 = 2 * common_background_information['W']\n",
    "        mu_2 = (h_1 + h_2) / 2\n",
    "        cov_diag_2 = 0.5 * common_background_information['W'] + data_specific_background_information['Sigma_plus']\n",
    "        result1 = multivariate_gaussian_pdf(h_1, mu_1, cov_diag_1)\n",
    "        result2 = multivariate_gaussian_pdf(data_specific_background_information['m_plus'], mu_2, cov_diag_2)\n",
    "        return (common_background_information['constant_c'] ** 2 ) * result1 * result2\n",
    "    \n",
    "    def expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        result_ = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_f_u'].reshape(1, -1), data_specific_background_information['input_K_f_u'].reshape(1, -1)).to_dense().data \n",
    "        result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype) @ common_background_information['m_u'].to(result_.dtype)\n",
    "        return result_\n",
    "        \n",
    "    def expectation_lambda_square(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        result_ = common_background_information['m_u']\n",
    "        _result = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype)\n",
    "        interm_term = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_u_f_K_f_u'], data_specific_background_information['input_K_u_f_K_f_u']).to_dense().data\n",
    "        result_ = _result @ interm_term.to(result_.dtype) @ _result.t()\n",
    "        # result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype) @ common_background_information['m_u']\n",
    "\n",
    "        if data_specific_background_information['expectation_K_uu'] == None:\n",
    "            data_specific_background_information['expectation_K_uu'] = interm_term\n",
    "        return result_\n",
    "        \n",
    "    def expectation_gamma(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        result_ = common_background_information['var_H'] * common_background_information['var_X']\n",
    "\n",
    "        if data_specific_background_information['expectation_K_uu'] == None:\n",
    "            data_specific_background_information['expectation_K_uu'] = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_u_f_K_f_u'], \\\n",
    "                                                                                                    data_specific_background_information['input_K_u_f_K_f_u']).to_dense().data\n",
    "\n",
    "        return result_ + (common_background_information['A'] * data_specific_background_information['expectation_K_uu']).sum()\n",
    "    \n",
    "    def integration_predictive_mean(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        return expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)\n",
    "\n",
    "\n",
    "    def integration_predictive_var(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "        return expectation_lambda_square(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information) \\\n",
    "            + expectation_gamma(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information) \\\n",
    "            - expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)**2\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    expectation_latent_K_f_u = Tensor([G(my_model.variational_strategy.inducing_points_latent.data[i]).item() for i in range(config['n_inducing_latent'])])\n",
    "    expectation_latent_K_u_f_K_f_u = Tensor([R(my_model.variational_strategy.inducing_points_latent.data[i], my_model.variational_strategy.inducing_points_latent.data[j]).item() \\\n",
    "                                            for j in range(config['n_inducing_latent']) for i in range(config['n_inducing_latent'])]).reshape(config['n_inducing_latent'], config['n_inducing_latent'])\n",
    "\n",
    "    data_specific_background_information['expectation_latent_K_f_u'] = expectation_latent_K_f_u\n",
    "    data_specific_background_information['expectation_latent_K_u_f_K_f_u'] = expectation_latent_K_u_f_K_f_u\n",
    "\n",
    "    return integration_predictive_mean(data_specific_background_information=data_specific_background_information), \\\n",
    "           integration_predictive_var(data_specific_background_information=data_specific_background_information)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1815]), tensor([18.3273]))\n",
      "(tensor([0.7284]), tensor([53.7530]))\n",
      "(tensor([1.1499]), tensor([15.6998]))\n",
      "(tensor([0.4806]), tensor([21.6984]))\n",
      "(tensor([-0.2372]), tensor([48.8876]))\n",
      "(tensor([-0.7895]), tensor([51.2051]))\n",
      "(tensor([-1.0141]), tensor([11.4194]))\n",
      "(tensor([0.0224]), tensor([0.2853]))\n",
      "(tensor([0.2942]), tensor([4.4255]))\n",
      "(tensor([-1.5231]), tensor([7.8596]))\n",
      "(tensor([-0.3446]), tensor([3488.4443]))\n",
      "(tensor([0.2836]), tensor([34025.7695]))\n",
      "(tensor([-0.5187]), tensor([22782.6660]))\n",
      "(tensor([1.0543]), tensor([5264.1099]))\n",
      "(tensor([0.3456]), tensor([5630.4941]))\n",
      "(tensor([-1.0292]), tensor([91.9700]))\n",
      "(tensor([-1.2787]), tensor([49.1571]))\n",
      "(tensor([-0.8351]), tensor([25.8974]))\n",
      "(tensor([-0.8117]), tensor([59.5811]))\n",
      "(tensor([-1.7658]), tensor([32.5957]))\n"
     ]
    }
   ],
   "source": [
    "output_index = 14\n",
    "for i in range(20):\n",
    "    test_input = Tensor([i-10])\n",
    "    print(integration_prediction_func(test_input, output_index, my_model, common_background_information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninput_K_f_u = my_model.covar_module_input(test_input, my_model.variational_strategy.inducing_points_input).to_dense().data\\nprint(input_K_f_u.shape)\\n\\ninput_K_u_f_K_f_u = input_K_f_u.t() @ input_K_f_u\\nprint(input_K_u_f_K_f_u.shape)\\n\\ni = 0 # test data index\\n\\ndata_specific_background_information = {\\n    'm_plus': my_model.X.q_mu.data[i],\\n    'Sigma_plus': my_model.X.q_log_sigma.exp().data[i],\\n    'input_K_f_u': input_K_f_u,\\n    'input_K_u_f_K_f_u': input_K_u_f_K_f_u,\\n    'expectation_K_uu': None\\n}\\n\\n'''\\ninput_K_f_u: 1 * #input_inducing_points \\ninput_K_u_f_K_f_u: #input_inducing_points  *  #input_inducing_points \\n'''\\ndef multivariate_gaussian_pdf(x, mu, cov):\\n    '''cov is a vector, representing all elements in the diagonal matrix'''\\n    k = mu.size(0)\\n    cov_det = cov.prod()\\n    cov_inv = torch.diag(1.0 / cov)\\n    norm_factor = torch.sqrt((2 * torch.pi) ** k * cov_det)\\n\\n    x_mu = x - mu\\n    result = torch.exp(-0.5 * x_mu @ cov_inv @ x_mu.t()) / norm_factor\\n    return result.item()\\n\\n\\ndef G(h:Tensor, common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\\n\\n    mu = data_specific_background_information['m_plus']\\n    cov_diag = data_specific_background_information['Sigma_plus'] + common_background_information['W']\\n    result = multivariate_gaussian_pdf(h, mu, cov_diag)\\n    return common_background_information['constant_c'] * result\\n\\n\\ndef R(h_1:Tensor, h_2:Tensor, common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\\n    mu_1 = h_2\\n    cov_diag_1 = 2 * common_background_information['W']\\n    mu_2 = (h_1 + h_2) / 2\\n    cov_diag_2 = 0.5 * common_background_information['W'] + data_specific_background_information['Sigma_plus']\\n    result1 = multivariate_gaussian_pdf(h_1, mu_1, cov_diag_1)\\n    result2 = multivariate_gaussian_pdf(data_specific_background_information['m_plus'], mu_2, cov_diag_2)\\n\\n    return (common_background_information['constant_c'] ** 2 ) * result1 * result2\\n\\nexpectation_latent_K_f_u = Tensor([G(my_model.variational_strategy.inducing_points_latent.data[i]).item() for i in range(config['n_inducing_latent'])])\\nexpectation_latent_K_u_f_K_f_u = Tensor([R(my_model.variational_strategy.inducing_points_latent.data[i], my_model.variational_strategy.inducing_points_latent.data[j]).item()                                          for j in range(config['n_inducing_latent']) for i in range(config['n_inducing_latent'])]).reshape(config['n_inducing_latent'], config['n_inducing_latent'])\\n\\ndata_specific_background_information['expectation_latent_K_f_u'] = expectation_latent_K_f_u\\ndata_specific_background_information['expectation_latent_K_u_f_K_f_u'] = expectation_latent_K_u_f_K_f_u\\n\\ndef expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\\n    result_ = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_f_u'].reshape(1, -1), data_specific_background_information['input_K_f_u'].reshape(1, -1)).to_dense().data \\n    result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype) @ common_background_information['m_u'].to(result_.dtype)\\n    return result_\\n    \\n\\ndef expectation_lambda_square(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\\n    result_ = common_background_information['m_u']\\n    result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype)\\n    interm_term = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_u_f_K_f_u'], data_specific_background_information['input_K_u_f_K_f_u']).to_dense().data\\n    result_ = result_ @ interm_term.to(result_.dtype) \\n    result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype) @ common_background_information['m_u']\\n\\n    if data_specific_background_information['expectation_K_uu'] == None:\\n        data_specific_background_information['expectation_K_uu'] = interm_term\\n    return result_\\n    \\n\\ndef expectation_gamma(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\\n    result_ = common_background_information['var_H'] * common_background_information['var_X']\\n\\n    if data_specific_background_information['expectation_K_uu'] == None:\\n        data_specific_background_information['expectation_K_uu'] = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_u_f_K_f_u'],                                                                                                   data_specific_background_information['input_K_u_f_K_f_u']).to_dense().data\\n    \\n    return result_ + (common_background_information['A'] * data_specific_background_information['expectation_K_uu']).sum()\\n\\ndef integration_predictive_mean(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\\n    return expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)\\n\\ndef integration_predictive_var(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\\n    return expectation_lambda_square(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)          + expectation_gamma(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)          - expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)**2\\n\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "input_K_f_u = my_model.covar_module_input(test_input, my_model.variational_strategy.inducing_points_input).to_dense().data\n",
    "print(input_K_f_u.shape)\n",
    "\n",
    "input_K_u_f_K_f_u = input_K_f_u.t() @ input_K_f_u\n",
    "print(input_K_u_f_K_f_u.shape)\n",
    "\n",
    "i = 0 # test data index\n",
    "\n",
    "data_specific_background_information = {\n",
    "    'm_plus': my_model.X.q_mu.data[i],\n",
    "    'Sigma_plus': my_model.X.q_log_sigma.exp().data[i],\n",
    "    'input_K_f_u': input_K_f_u,\n",
    "    'input_K_u_f_K_f_u': input_K_u_f_K_f_u,\n",
    "    'expectation_K_uu': None\n",
    "}\n",
    "\n",
    "'''\n",
    "input_K_f_u: 1 * #input_inducing_points \n",
    "input_K_u_f_K_f_u: #input_inducing_points  *  #input_inducing_points \n",
    "'''\n",
    "def multivariate_gaussian_pdf(x, mu, cov):\n",
    "    '''cov is a vector, representing all elements in the diagonal matrix'''\n",
    "    k = mu.size(0)\n",
    "    cov_det = cov.prod()\n",
    "    cov_inv = torch.diag(1.0 / cov)\n",
    "    norm_factor = torch.sqrt((2 * torch.pi) ** k * cov_det)\n",
    "\n",
    "    x_mu = x - mu\n",
    "    result = torch.exp(-0.5 * x_mu @ cov_inv @ x_mu.t()) / norm_factor\n",
    "    return result.item()\n",
    "\n",
    "\n",
    "def G(h:Tensor, common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "\n",
    "    mu = data_specific_background_information['m_plus']\n",
    "    cov_diag = data_specific_background_information['Sigma_plus'] + common_background_information['W']\n",
    "    result = multivariate_gaussian_pdf(h, mu, cov_diag)\n",
    "    return common_background_information['constant_c'] * result\n",
    "\n",
    "\n",
    "def R(h_1:Tensor, h_2:Tensor, common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "    mu_1 = h_2\n",
    "    cov_diag_1 = 2 * common_background_information['W']\n",
    "    mu_2 = (h_1 + h_2) / 2\n",
    "    cov_diag_2 = 0.5 * common_background_information['W'] + data_specific_background_information['Sigma_plus']\n",
    "    result1 = multivariate_gaussian_pdf(h_1, mu_1, cov_diag_1)\n",
    "    result2 = multivariate_gaussian_pdf(data_specific_background_information['m_plus'], mu_2, cov_diag_2)\n",
    "\n",
    "    return (common_background_information['constant_c'] ** 2 ) * result1 * result2\n",
    "\n",
    "expectation_latent_K_f_u = Tensor([G(my_model.variational_strategy.inducing_points_latent.data[i]).item() for i in range(config['n_inducing_latent'])])\n",
    "expectation_latent_K_u_f_K_f_u = Tensor([R(my_model.variational_strategy.inducing_points_latent.data[i], my_model.variational_strategy.inducing_points_latent.data[j]).item() \\\n",
    "                                         for j in range(config['n_inducing_latent']) for i in range(config['n_inducing_latent'])]).reshape(config['n_inducing_latent'], config['n_inducing_latent'])\n",
    "\n",
    "data_specific_background_information['expectation_latent_K_f_u'] = expectation_latent_K_f_u\n",
    "data_specific_background_information['expectation_latent_K_u_f_K_f_u'] = expectation_latent_K_u_f_K_f_u\n",
    "\n",
    "def expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "    result_ = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_f_u'].reshape(1, -1), data_specific_background_information['input_K_f_u'].reshape(1, -1)).to_dense().data \n",
    "    result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype) @ common_background_information['m_u'].to(result_.dtype)\n",
    "    return result_\n",
    "    \n",
    "\n",
    "def expectation_lambda_square(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "    result_ = common_background_information['m_u']\n",
    "    result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype)\n",
    "    interm_term = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_u_f_K_f_u'], data_specific_background_information['input_K_u_f_K_f_u']).to_dense().data\n",
    "    result_ = result_ @ interm_term.to(result_.dtype) \n",
    "    result_ = result_ @ common_background_information['chol_K_uu_inv_t'].to(result_.dtype) @ common_background_information['m_u']\n",
    "\n",
    "    if data_specific_background_information['expectation_K_uu'] == None:\n",
    "        data_specific_background_information['expectation_K_uu'] = interm_term\n",
    "    return result_\n",
    "    \n",
    "\n",
    "def expectation_gamma(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "    result_ = common_background_information['var_H'] * common_background_information['var_X']\n",
    "\n",
    "    if data_specific_background_information['expectation_K_uu'] == None:\n",
    "        data_specific_background_information['expectation_K_uu'] = KroneckerProductLinearOperator(data_specific_background_information['expectation_latent_K_u_f_K_f_u'], \\\n",
    "                                                                                                  data_specific_background_information['input_K_u_f_K_f_u']).to_dense().data\n",
    "    \n",
    "    return result_ + (common_background_information['A'] * data_specific_background_information['expectation_K_uu']).sum()\n",
    "\n",
    "def integration_predictive_mean(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "    return expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)\n",
    "\n",
    "def integration_predictive_var(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information):\n",
    "    return expectation_lambda_square(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information) \\\n",
    "         + expectation_gamma(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information) \\\n",
    "         - expectation_lambda(common_background_information=common_background_information, data_specific_background_information=data_specific_background_information)**2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPLVM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
