{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/')\n",
    "import yaml\n",
    "import torch\n",
    "# from models_.lvmogp_svi import LVMOGP_SVI\n",
    "from modules.prepare_and_train_model import LVMOGP_SVI \n",
    "from models_.gaussian_likelihood import GaussianLikelihood\n",
    "from modules.prepare_data import *\n",
    "from util_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model and likelihood\n",
    "\n",
    "with open('/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/configs/train_config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "my_model = LVMOGP_SVI(\n",
    "    n_latent = config['n_latent'],\n",
    "    n_input = config['n_input_train'],\n",
    "    input_dim = config['input_dim'],\n",
    "    latent_dim = config['latent_dim'],\n",
    "    n_inducing_input = config['n_inducing_input'],\n",
    "    n_inducing_latent = config['n_inducing_latent'],\n",
    "    data_Y = None,\n",
    "    pca = config['pca'],\n",
    "    learn_inducing_locations_latent = config['learn_inducing_locations_latent'],\n",
    "    learn_inducing_locations_input = config['learn_inducing_locations_input'],\n",
    "    latent_kernel_type = config['latent_kernel_type'],\n",
    "    input_kernel_type = config['input_kernel_type']\n",
    ")\n",
    "\n",
    "my_likelihood = GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model and likelihood\n",
    "\n",
    "model_state_dict = torch.load(config['model_path'])\n",
    "my_model.load_state_dict(model_state_dict)\n",
    "\n",
    "likelihood_state_dict = torch.load(config['likelihood_path'])\n",
    "my_likelihood.load_state_dict(likelihood_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dataset_type'] == 'synthetic_regression':\n",
    "        data_inputs, data_Y_squeezed, ls_of_ls_train_input, ls_of_ls_test_input, train_sample_idx_ls, test_sample_idx_ls = prepare_synthetic_regression_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first evaluate on all data, including train and test ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.eval()\n",
    "my_likelihood.eval()\n",
    "\n",
    "all_index_latent = np.array([[i]*config['n_input'] for i in range(config['n_latent'])]).reshape(-1).tolist() \n",
    "all_index_input = [i for i in range(config['n_input'])] * config['n_latent'] \n",
    "len_latent = len(all_index_latent)\n",
    "assert len_latent == len(all_index_input)\n",
    "all_mean_latent = my_model.X.q_mu\n",
    "\n",
    "test_mini_batch_size = 1000\n",
    "\n",
    "all_pred_mean = torch.zeros(len_latent)\n",
    "all_pred_var = torch.zeros(len_latent)\n",
    "test_continue = True\n",
    "test_start_idx = 0\n",
    "test_end_idx = test_mini_batch_size\n",
    "\n",
    "# iteratively inference\n",
    "while test_continue:\n",
    "    batch_latent = all_mean_latent[all_index_latent[test_start_idx:test_end_idx]]\n",
    "    batch_input = data_inputs[all_index_input[test_start_idx:test_end_idx]]\n",
    "    batch_output = my_likelihood(my_model(batch_latent, batch_input))\n",
    "    all_pred_mean[test_start_idx:test_end_idx] = batch_output.loc.detach()\n",
    "    all_pred_var[test_start_idx:test_end_idx] = batch_output.variance.detach()\n",
    "\n",
    "    if test_end_idx < len_latent:\n",
    "        test_start_idx += test_mini_batch_size\n",
    "        test_end_idx += test_mini_batch_size\n",
    "        test_end_idx = min(test_end_idx, len_latent)\n",
    "    else:\n",
    "        test_continue = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finer grid for better visualization ... nothing to do with RMSE computation ... \n",
    "n_data4visual = 500\n",
    "inputs_total4visual = Tensor(np.linspace(config['init_inducing_input_LB'], config['init_inducing_input_UB'], n_data4visual))\n",
    "all_index_latent4visual = np.array([[i]*n_data4visual for i in range(config['n_latent'])]).reshape(-1).tolist() \n",
    "all_index_input4visual = [i for i in range(n_data4visual)] * config['n_latent'] \n",
    "\n",
    "len_latent4visual = len(all_index_latent4visual)\n",
    "assert len_latent4visual == len(all_index_input4visual)\n",
    "\n",
    "all_pred_mean4visual = torch.zeros(len_latent4visual)\n",
    "all_pred_var4visual = torch.zeros(len_latent4visual)\n",
    "\n",
    "test_continue = True\n",
    "test_start_idx = 0\n",
    "test_end_idx = test_mini_batch_size\n",
    "\n",
    "while test_continue:\n",
    "    batch_latent = all_mean_latent[all_index_latent4visual[test_start_idx:test_end_idx]]\n",
    "    batch_input = inputs_total4visual[all_index_input4visual[test_start_idx:test_end_idx]]\n",
    "    batch_output = my_likelihood(my_model(batch_latent, batch_input))\n",
    "    all_pred_mean4visual[test_start_idx:test_end_idx] = batch_output.loc.detach()\n",
    "    all_pred_var4visual[test_start_idx:test_end_idx] = batch_output.variance.detach()\n",
    "\n",
    "    if test_end_idx < len_latent4visual:\n",
    "        test_start_idx += test_mini_batch_size\n",
    "        test_end_idx += test_mini_batch_size\n",
    "        test_end_idx = min(test_end_idx, len_latent4visual)\n",
    "    else:\n",
    "        test_continue = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test data RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_predict = all_pred_mean[train_sample_idx_ls]\n",
    "train_rmse = (train_data_predict - data_Y_squeezed[train_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Train RMSE', train_rmse)\n",
    "\n",
    "w_test_data_predict = all_pred_mean[test_sample_idx_ls]\n",
    "test_rmse = (w_test_data_predict - data_Y_squeezed[test_sample_idx_ls]).square().mean().sqrt()\n",
    "print('Global Test RMSE', test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test data NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nll = neg_log_likelihood(Target=data_Y_squeezed[train_sample_idx_ls], GaussianMean=all_pred_mean[train_sample_idx_ls], GaussianVar=all_pred_var[train_sample_idx_ls])\n",
    "test_nll = neg_log_likelihood(Target=data_Y_squeezed[test_sample_idx_ls], GaussianMean=all_pred_mean[test_sample_idx_ls], GaussianVar=all_pred_var[test_sample_idx_ls])\n",
    "\n",
    "print('Global Train negative log likelihood:', train_nll)\n",
    "print('Global Test negative log likelihood:', test_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_simgle_output(w_function_index):\n",
    "    # Pick the index of the funtion to show\n",
    "    # w_function_index = 982\n",
    "\n",
    "    performance_dirct = {}\n",
    "    w_train_input = data_inputs[ls_of_ls_train_input[w_function_index]]\n",
    "    w_train_start = 0\n",
    "    for i in range(w_function_index):\n",
    "        w_train_start += len(ls_of_ls_train_input[i]) # don't assume every output has the same length of inputs\n",
    "    w_train_end = w_train_start + len(ls_of_ls_train_input[w_function_index])\n",
    "    w_train_target = data_Y_squeezed[train_sample_idx_ls][w_train_start:w_train_end]\n",
    "    w_train_predict = train_data_predict[w_train_start:w_train_end]\n",
    "    train_rmse_ = (w_train_target - w_train_predict).square().mean().sqrt()\n",
    "    train_nll_ = neg_log_likelihood(w_train_target, all_pred_mean[train_sample_idx_ls][w_train_start:w_train_end], all_pred_var[train_sample_idx_ls][w_train_start:w_train_end])\n",
    "    # print('train rmse', train_rmse_)\n",
    "    # print('train nll:', train_nll_)\n",
    "    performance_dirct['train_rmse'] = train_rmse_\n",
    "    performance_dirct['train_nll'] = train_nll_\n",
    "\n",
    "    w_test_input = data_inputs[ls_of_ls_test_input[w_function_index]]\n",
    "    w_test_start = 0\n",
    "    for j in range(w_function_index):\n",
    "        w_test_start += len(ls_of_ls_test_input[i])\n",
    "    w_test_end = w_test_start + len(ls_of_ls_test_input[w_function_index])\n",
    "    w_test_target = data_Y_squeezed[test_sample_idx_ls][w_test_start:w_test_end]\n",
    "    w_test_predict = w_test_data_predict[w_test_start:w_test_end]\n",
    "    test_rmse_ = (w_test_predict - w_test_target).square().mean().sqrt()\n",
    "    test_nll_ = neg_log_likelihood(w_test_target, all_pred_mean[test_sample_idx_ls][w_test_start:w_test_end], all_pred_var[test_sample_idx_ls][w_test_start:w_test_end])\n",
    "    # print('test rmse', test_rmse_)\n",
    "    # print('test nll', test_nll_)\n",
    "    performance_dirct['test_rmse'] = test_rmse_\n",
    "    performance_dirct['test_nll'] = test_nll_\n",
    "\n",
    "    w_gp_input = data_inputs\n",
    "    w_gp_start = w_gp_input.shape[0] * w_function_index\n",
    "    w_gp_end = w_gp_start + w_gp_input.shape[0]\n",
    "    w_gp_target = data_Y_squeezed[w_gp_start:w_gp_end]\n",
    "\n",
    "    # NOTE: comment these since bad visualization ... \n",
    "    # w_gp_pred_mean = all_pred_mean[w_gp_start:w_gp_end]\n",
    "    # w_gp_pred_std = all_pred_var[w_gp_start:w_gp_end]\n",
    "\n",
    "    w_gp4visual_start = n_data4visual * w_function_index\n",
    "    w_gp4visual_end = n_data4visual * (w_function_index + 1)\n",
    "    w_gp_pred_mean = all_pred_mean4visual[w_gp4visual_start:w_gp4visual_end]\n",
    "    w_gp_pred_std = all_pred_var4visual.sqrt()[w_gp4visual_start:w_gp4visual_end]\n",
    "\n",
    "    return w_train_input, w_train_target, w_test_input, w_test_target, w_gp_pred_mean, w_gp_pred_std, performance_dirct\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over all function index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse_list = [] # list of tensors\n",
    "test_rmse_list = []\n",
    "train_nll_list = []\n",
    "test_nll_list = []\n",
    "for output_index in range(config['n_latent']):\n",
    "    _, _, _, _, _, _, performance_dirct = evaluate_on_simgle_output(output_index)\n",
    "    train_rmse_list.append(performance_dirct['train_rmse'])\n",
    "    test_rmse_list.append(performance_dirct['test_rmse'])\n",
    "    train_nll_list.append(performance_dirct['train_nll'])\n",
    "    test_nll_list.append(performance_dirct['test_nll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median_index(lst):\n",
    "    sorted_lst = sorted(lst)\n",
    "    n = len(lst)\n",
    "    \n",
    "    if n % 2 != 0:\n",
    "        median = sorted_lst[n // 2]\n",
    "        return lst.index(median)\n",
    "    else:\n",
    "        mid1 = sorted_lst[n // 2 - 1]\n",
    "        mid2 = sorted_lst[n // 2]\n",
    "        \n",
    "        return lst.index(mid1)  # lst.index(mid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The output index with WORSE test rmse performance: ', test_rmse_list.index(max(test_rmse_list)))\n",
    "print('The output index with WORSE test nll performance: ', test_nll_list.index(max(test_nll_list)))\n",
    "print('------' * 10)\n",
    "print('The output index with MIDDLE test rmse performance:', find_median_index(test_rmse_list))\n",
    "print('The output index with MIDDLE test nll performance:', find_median_index(test_nll_list))\n",
    "print('------' * 10)\n",
    "print('The output index with BEST test rmse performance: ', test_rmse_list.index(min(test_rmse_list)))\n",
    "print('The output index with BEST test nll performance: ', test_nll_list.index(min(test_nll_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_index = 375\n",
    "w_train_input, w_train_target, w_test_input, w_test_target, w_gp_pred_mean, w_gp_pred_std, performance_dirct = evaluate_on_simgle_output(function_index)\n",
    "picture_save_path = f'/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/experi_results/func_id_{function_index}_numofoutput_{config[\"n_latent\"]}.png'\n",
    "plot_traindata_testdata_fittedgp(train_X=w_train_input, train_Y=w_train_target, test_X=w_test_input, test_Y=w_test_target, gp_X=inputs_total4visual, gp_pred_mean=w_gp_pred_mean, gp_pred_std=w_gp_pred_std, inducing_points_X=my_model.variational_strategy.inducing_points_input.data, n_inducing_C=config['n_inducing_input'], picture_save_path=picture_save_path) # NOTE: input is C not X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_model.covar_module_latent.outputscale.detach())\n",
    "print(my_model.covar_module_latent.base_kernel.lengthscale.detach())\n",
    "print(my_model.covar_module_input.outputscale.detach())\n",
    "print(my_model.covar_module_input.base_kernel.lengthscale.detach())\n",
    "print(my_likelihood.noise.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPLVM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
